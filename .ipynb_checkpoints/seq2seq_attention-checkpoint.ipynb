{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.preprocessing data\n",
    "# 2.build model\n",
    "# 2.1 encoder\n",
    "# 2.2 attention\n",
    "# 2.3 decoder\n",
    "# 2.4 loss & optimizer\n",
    "# 2.5 train\n",
    "# 3. evaluation\n",
    "# 3.1 given sentence,return translated results\n",
    "# 3.2 visualize results (attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "May I borrow this book?\n",
      "Tomatelo con soda.\n"
     ]
    }
   ],
   "source": [
    "en_spa_file_path = './spa.txt'\n",
    "import unicodedata\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD',s) if unicodedata.category(c)!='Mn')\n",
    "\n",
    "en_sentence = u\"May I borrow this book?\"\n",
    "sp_sentence = u\"Tomátelo con soda.\"\n",
    "print(unicode_to_ascii(en_sentence))\n",
    "print(unicode_to_ascii(sp_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> may i borrow this book ? <end>\n",
      "<start> tomatelo con soda . <end>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def preprocess_sentence(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    # 标点符号前后加空格\n",
    "    s = re.sub(r\"([?.!,¿])\",r\" \\1 \",s)\n",
    "    # 多余空格变一个空格 \n",
    "    s = re.sub(r'[\" \"]+',\" \",s)\n",
    "    # 除了标点符号和字母外都是空格\n",
    "    s = re.sub(r'[^a-zA-Z?.!,¿]',' ',s)\n",
    "    # 去掉前后空格\n",
    "    s = s.rstrip().strip()\n",
    "    s = '<start> '+s+ ' <end>'\n",
    "    return s\n",
    "print(preprocess_sentence(en_sentence))\n",
    "print(preprocess_sentence(sp_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> if you want to sound like a native speaker , you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo . <end>\n",
      "<start> si quieres sonar como un hablante nativo , debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un musico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado . <end>\n",
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "def perse_data(filename):\n",
    "    lines = open(filename,encoding='UTF-8').read().strip().split('\\n')\n",
    "    sentence_pairs = [line.split('\\t') for line in lines ]\n",
    "    preprocessed_sentence_pairs = [\n",
    "        (preprocess_sentence(en),preprocess_sentence(sp)) for en,sp in sentence_pairs ]\n",
    "    return zip(*preprocessed_sentence_pairs)\n",
    "en_dataset,sp_dataset =  perse_data(en_spa_file_path)\n",
    "print(en_dataset[-1])\n",
    "print(sp_dataset[-1])\n",
    "print(type(en_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 5) (2, 4, 6)\n"
     ]
    }
   ],
   "source": [
    "a = [(1,2),(3,4),(5,6)]\n",
    "c, d = zip(*a)\n",
    "print(c,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 16)\n",
      "(30000, 11)\n",
      "16 11\n"
     ]
    }
   ],
   "source": [
    "def tokenizer(lang):\n",
    "    lang_tokenizer = keras.preprocessing.text.Tokenizer(num_words=None,filters='',split=' ')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    # maxlen=None 默认序列的最大长度\n",
    "    tensor = keras.preprocessing.sequence.pad_sequences(tensor,padding = 'post')\n",
    "    return tensor ,lang_tokenizer\n",
    "input_tensor,input_tokenizer = tokenizer(sp_dataset[0:30000])\n",
    "output_tensor,output_tokenizer = tokenizer(en_dataset[0:30000])\n",
    "\n",
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "max_length_input = max_length(input_tensor)\n",
    "max_length_output = max_length(output_tensor)\n",
    "print(input_tensor.shape)\n",
    "print(output_tensor.shape)\n",
    "print(max_length_input,max_length_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24000, 6000, 24000, 6000)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "input_train,input_eval,output_train,output_eval = train_test_split(input_tensor,output_tensor,test_size = 0.2)\n",
    "len(input_train),len(input_eval),len(output_train),len(output_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 --><start>\n",
      "4 -->tom\n",
      "55 -->fue\n",
      "1009 -->despedido\n",
      "3 -->.\n",
      "2 --><end>\n",
      "\n",
      "1 --><start>\n",
      "5 -->tom\n",
      "51 -->has\n",
      "160 -->been\n",
      "502 -->fired\n",
      "3 -->.\n",
      "2 --><end>\n"
     ]
    }
   ],
   "source": [
    "def convert(example,tokenizer):\n",
    "    for t in example:\n",
    "        if t != 0:\n",
    "            print('%d -->%s'%(t,tokenizer.index_word[t]))\n",
    "convert(input_train[0],input_tokenizer)\n",
    "print()\n",
    "convert(output_train[0],output_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(input_tensor,output_tensor,batch_size,epochs,shuffle):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((input_tensor,output_tensor))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(30000)\n",
    "    dataset = dataset.repeat(epochs).batch(batch_size,drop_remainder = True)\n",
    "    return dataset\n",
    "batch_size =64\n",
    "epochs =20\n",
    "train_dataset = make_dataset(input_train,output_train,batch_size,epochs,True)\n",
    "eval_dataset = make_dataset(input_eval,output_eval,batch_size,1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_units = 256\n",
    "units = 1024\n",
    "# 字典长度加1  为补齐的0向量\n",
    "input_vocab_size = len(input_tokenizer.word_index)+1\n",
    "output_vocab_size = len(output_tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_output.shape: (64, 16, 1024)\n",
      "sample_hidden.shape: (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "class Encoder(keras.Model):\n",
    "    def __init__(self,vocab_size,embedding_units,encoding_units,batch_size):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.encoding_units = encoding_units\n",
    "        self.embedding = keras.layers.Embedding(vocab_size,embedding_units)\n",
    "        self.gru = keras.layers.GRU(self.encoding_units,\n",
    "                                    return_sequences = True,\n",
    "                                    return_state = True,\n",
    "                                    recurrent_initializer = 'glorot_uniform')\n",
    "    def call(self,x,hidden):\n",
    "        x = self.embedding(x)\n",
    "        output,state = self.gru(x,initial_state = hidden)\n",
    "        return output,state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size,self.encoding_units))\n",
    "encoder = Encoder(input_vocab_size,embedding_units,units,batch_size)\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "# return_sequence返回每个时间步上的隐状态（hidden state），return_state返回最后一个时间步上细胞状态（cell state）\n",
    "sample_output,sample_hidden = encoder(x,sample_hidden)\n",
    "print(\"sample_output.shape:\",sample_output.shape)\n",
    "print(\"sample_hidden.shape:\",sample_hidden.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_results.shape: (64, 1024)\n",
      "attention_weights.shape: (64, 16, 1)\n"
     ]
    }
   ],
   "source": [
    "class BahdanauAttention(keras.Model):\n",
    "    def __init__(self,units):\n",
    "        super(BahdanauAttention,self).__init__()\n",
    "        self.W1 = keras.layers.Dense(units)\n",
    "        self.W2 = keras.layers.Dense(units)\n",
    "        self.V = keras.layers.Dense(1)\n",
    "    def call(self,decoder_hidden,encoder_outputs):\n",
    "        # decoder_hidden.shape: (bath_size,units)\n",
    "        # encoder_outputs.shape: (batch_size,length,units)\n",
    "        # 维度扩展 ecoder_hidden.shape: (bath_size,1，units)\n",
    "        decoder_hidden_with_time_axis = tf.expand_dims(decoder_hidden,1)\n",
    "        # before V: (batch_size,length.units)\n",
    "        # ather V: (batch_size,length,1)\n",
    "        # print(self.W1(encoder_outputs).shape)  (64, 16, 10)\n",
    "        # print(self.W2(decoder_hidden_with_time_axis).shape)   (64, 1, 10)\n",
    "        score = self.V(tf.nn.tanh(self.W1(encoder_outputs) + self.W2(decoder_hidden_with_time_axis)))\n",
    "        # print(score.shape) (64, 16, 1)\n",
    "        # shape: (batch_size,length,1)\n",
    "        attention_weights = tf.nn.softmax(score,axis = 1)\n",
    "        # context_vector (batch_size,length,units)\n",
    "        context_vector = attention_weights * encoder_outputs\n",
    "        # context_vector.shape: (batch_size,units)\n",
    "        context_vector = tf.reduce_sum(context_vector,axis =1 )\n",
    "        return context_vector,attention_weights\n",
    "# 测试\n",
    "attention_model = BahdanauAttention(units = 10)\n",
    "attention_results,attention_weights = attention_model(sample_hidden,sample_output)\n",
    "print(\"attention_results.shape:\",attention_results.shape)\n",
    "print(\"attention_weights.shape:\",attention_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_output.shape: (64, 4935)\n",
      "decoder_hidden.shape: (64, 1024)\n",
      "decoder_aw.shape (64, 16, 1)\n"
     ]
    }
   ],
   "source": [
    "class Decoder(keras.Model):\n",
    "    def __init__(self,vocab_size,embedding_dim,decoding_units,batch_size):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.decoding_units = decoding_units\n",
    "        self.embedding = keras.layers.Embedding(vocab_size,embedding_units)\n",
    "        self.gru = keras.layers.GRU(self.decoding_units,\n",
    "                                    return_sequences= True,\n",
    "                                    return_state = True,\n",
    "                                    recurrent_initializer = 'glorot_uniform')\n",
    "        self.fc = keras.layers.Dense(vocab_size)\n",
    "        self.attention = BahdanauAttention(self.decoding_units)\n",
    "    def call(self,x,hidden,encoding_outputs):\n",
    "        # context_vector.shape: (batch_size,units)\n",
    "        context_vector,attention_weights = self.attention(hidden,encoding_outputs)\n",
    "        # x 为单词\n",
    "        # before embedding : x.shape: (batch_size,1)\n",
    "        # after embedding : x.shape: (batch_size,1,embedding_units)\n",
    "        x = self.embedding(x)\n",
    "        combined_x = tf.concat([tf.expand_dims(context_vector,1),x],axis=-1)\n",
    "        # print(combined_x.shape)  (64, 1, 1280)  1024+256\n",
    "        # output.shape: [batch_size,1,decoding_units]\n",
    "        # state.shape : [batch_size,decoding_units]\n",
    "        output,state = self.gru(combined_x)\n",
    "        # output.shape: [batch_size,decoding_units]\n",
    "        output = tf.reshape(output,(-1,output.shape[2]))\n",
    "        # output.shape: [batch_size,vocab_size]\n",
    "        output = self.fc(output)\n",
    "        return output,state,attention_weights\n",
    "\n",
    "decoder = Decoder(output_vocab_size,embedding_units,units,batch_size)\n",
    "outputs = decoder(tf.random.uniform((batch_size,1)),sample_hidden,sample_output)\n",
    "decoder_output,decoder_hidden,decoder_aw = outputs\n",
    "print(\"decoder_output.shape:\",decoder_output.shape)\n",
    "print(\"decoder_hidden.shape:\",decoder_hidden.shape)\n",
    "print(\"decoder_aw.shape\",decoder_aw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam()\n",
    "loss_object = keras.losses.SparseCategoricalCrossentropy(from_logits=True,reduction='none')\n",
    "\n",
    "def loss_function(real,pred):\n",
    "    # targ很多padding为0的部分 将这些部分不加入损失函数计算\n",
    "    mask = tf.math.logical_not(tf.math.equal(real,0))\n",
    "    # 计算出每个targ对应的损失\n",
    "    loss_ = loss_object(real,pred)\n",
    "    mask = tf.cast(mask,dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    print(loss_)\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0.6897267], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "y_true = [1]\n",
    "y_pred = [[0.1, 0.8, 0.1]]\n",
    "loss_v = loss_object(y_true,y_pred)\n",
    "print(loss_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0.6897267], shape=(1,), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.6897267>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_function(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp,targ,encoding_hidden):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        encoding_outputs, encoding_hidden = encoder(inp,encoding_hidden)\n",
    "        decoding_hidden = encoding_hidden \n",
    "        # eg: <start> I am here <end>\n",
    "        # 1. <start> -> I\n",
    "        # 2. I -> am\n",
    "        # 3. am -> here\n",
    "        # 4. here -> <end>\n",
    "        for t in range(0,targ.shape[1]-1):\n",
    "            decoding_input = tf.expand_dims(targ[:,t],1)\n",
    "            predictions,decoding_hidden, _ = decoder(decoding_input,decoding_hidden,encoding_outputs)\n",
    "            loss += loss_function(targ[:,t+1],predictions)\n",
    "    # 计算平均的损失函数\n",
    "    batch_loss = loss /int(targ.shape[0])\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss,variables)\n",
    "    optimizer.apply_gradients(zip(gradients,variables))\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"mul:0\", shape=(64,), dtype=float32)\n",
      "Tensor(\"mul_1:0\", shape=(64,), dtype=float32)\n",
      "Tensor(\"mul_2:0\", shape=(64,), dtype=float32)\n",
      "Tensor(\"mul_3:0\", shape=(64,), dtype=float32)\n",
      "Tensor(\"mul_4:0\", shape=(64,), dtype=float32)\n",
      "Tensor(\"mul_5:0\", shape=(64,), dtype=float32)\n",
      "Tensor(\"mul_6:0\", shape=(64,), dtype=float32)\n",
      "Tensor(\"mul_7:0\", shape=(64,), dtype=float32)\n",
      "Tensor(\"mul_8:0\", shape=(64,), dtype=float32)\n",
      "Tensor(\"mul_9:0\", shape=(64,), dtype=float32)\n",
      "Tensor(\"mul:0\", shape=(64,), dtype=float32)\n",
      "Tensor(\"mul_1:0\", shape=(64,), dtype=float32)\n",
      "Tensor(\"mul_2:0\", shape=(64,), dtype=float32)\n",
      "Tensor(\"mul_3:0\", shape=(64,), dtype=float32)\n",
      "Tensor(\"mul_4:0\", shape=(64,), dtype=float32)\n",
      "Tensor(\"mul_5:0\", shape=(64,), dtype=float32)\n",
      "Tensor(\"mul_6:0\", shape=(64,), dtype=float32)\n",
      "Tensor(\"mul_7:0\", shape=(64,), dtype=float32)\n",
      "Tensor(\"mul_8:0\", shape=(64,), dtype=float32)\n",
      "Tensor(\"mul_9:0\", shape=(64,), dtype=float32)\n",
      "Epoch1 Batch0 Loss0.7785\n",
      "Epoch1 Batch100 Loss0.3679\n",
      "Epoch1 Batch200 Loss0.3177\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "epochs = 10 \n",
    "steps_per_epoch = len(input_tensor) // batch_size\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    encoding_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    for (batch,(inp,targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp,targ,encoding_hidden)\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        if batch%100 ==0:\n",
    "            print('Epoch{} Batch{} Loss{:.4f}'.format(epoch+1,batch,batch_loss.numpy()))\n",
    "    print('Epoch{} Loss{:.4f}'.format(epoch+1,total_loss/steps_per_epoch))\n",
    "    print('Time take for epoch{} sec\\n'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "def evaluate(input_sentence):\n",
    "    attention_matrix = np.zeros((max_length_output,max_length_input))\n",
    "    input_sentence = preprocess_sentence(input_sentence)\n",
    "    \n",
    "    inputs = [input_tokenizer.word_index[token] for token in input_sentence.split(' ')]\n",
    "    inputs = keras.preprocessing.sequence.pad_sequences([inputs],maxlen = max_length_input,padding= 'post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    results = ''\n",
    "    # encoding_hidden = encoder.initialize_hidden_state()\n",
    "    encoding_hidden = tf.zeros((1,units))\n",
    "    encoding_outputs ,encoding_hidden = encoder(inputs,encoding_hidden)\n",
    "    decoding_hidden = encoding_hidden\n",
    "    \n",
    "    # eg: <start> -> A\n",
    "    # A -> B -> C-> D\n",
    "    \n",
    "    # decoding_input.shape: (1,1)\n",
    "    decoding_input = tf.expand_dims([output_tokenizer.word_index['<start>']],0)\n",
    "    \n",
    "    for t in range(max_length_output):\n",
    "        predictions,decoding_hidden,attention_weights = decoder(decoding_input,decoding_hidden,encoding_outputs)\n",
    "        # attention_weights.shape: (batch_size,input_length,1) (1,16,1)\n",
    "        # 存储注意力权重以便后面制图\n",
    "        attention_weights = tf.reshape(attention_weights,(-1,))\n",
    "        attention_matrix[t] = attention_weights.numpy()\n",
    "        \n",
    "        # predictions.shape:(batch_size,vocab_size)  (1,4935)\n",
    "        print(\"...\",predictions[0])\n",
    "        print(\"re..\",tf.reduce_max(predictions[0]))\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        results += output_tokenizer.index_word[predicted_id]+ ' '\n",
    "        if output_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return results,input_sentence,attention_matrix\n",
    "        # 上一步的输出是下一步的输入\n",
    "        decoding_input = tf.expand_dims([predicted_id],0)\n",
    "    return results,inputs,input_sentence,attention_matrix\n",
    "\n",
    "def plot_attention(attention_matrix,input_sentence,predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.matshow(attention_matrix,cmap='viridis')\n",
    "    \n",
    "    font_dict = {'fontsize':14}\n",
    "    ax.set_xticklabels(['']+input_sentence,fontdict = font_dict,rotation = 90 )\n",
    "    ax.set_yticklabels(['']+predicted_sentence,fontdict = font_dict,)\n",
    "    plt.show()\n",
    "\n",
    "def translate(input_sentence):\n",
    "    results,input_sentence,attention_matrix = evaluate(input_sentence)\n",
    "    print(\"Input:%s\"%(input_sentence))\n",
    "    print(\"Predicted translation:%s\"%(results))\n",
    "    attention_matrix = attention_matrix[:len(results.split(' ')),:len(input_sentence.split(' '))]\n",
    "    plot_attention(attention_matrix,input_sentence.split(' '),results.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(u'¿todavia estan en casa?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
